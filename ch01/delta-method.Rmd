---
title: Univariate delta method example
author: Sean Anderson
output: pdf_document
---

This is an example of using the delta method to derive the standard error of a transformed parameter.

See Jay Ver Hoef's paper:
Who Invented the Delta Method?
<https://doi.org/10.1080/00031305.2012.687494>

Lets imagine we have some parameter 'theta' from a model.
Theta could be a slope coefficient from a linear regression.
Now, say we wanted to calculate a confidence interval on some transformed version of theta.
Here we will pretend we want to calculate a confidence interval, and hence standard error, on log(theta).

Our theta (estimated parameter) here will have a value of 10 and a standard error of 0.8. We'll simulate from 'theta' so we have some samples to work with for comparison to prove things to ourselves:

```{r}
set.seed(1)
param_samples <- rnorm(1e6, 10, 0.8)
hist(param_samples, breaks = 80)
```

Why a normal distribution? Because our models typically assume estimated
parameters can be approximated well by a normal distribution. This goes
back to the Central Limit Theorem, which essentially says the distribution
of means from any distribution is itself normal. So, the distribution
of a parameter estimate (which is a distribution on a mean) should
be normally distributed given sufficient data.

Pretend this is our parameter 'estimate' from some model:

```{r}
theta <- mean(param_samples)
theta
```

And pretend this is our standard error of our parameter estimate:

```{r}
theta_se <- sd(param_samples)
theta_se
```

Say we wanted to find the standard error of the log of our parameter.
We will represent this transformation function as g: g(x) = log(x)

We can get the mean easily:

```{r}
g_theta <- log(theta)
g_theta
hist(log(param_samples), breaks = 80)
abline(v = g_theta, col = "red")
```

And we can confirm this is right, because we have some samples from the parameter to prove it to ourselves:

```{r}
mean(log(param_samples))
```

But how do we find the standard error on this transformed parameter? For that we can use the delta method. The high-level intuition would be that the delta method uses math from a 1st order [Taylor series expansion](https://en.wikipedia.org/wiki/Taylor_series) to calculate the variance of a transformed parameter based on the variance of the original parameter estimate and that transformation function. Also see [Ver Hoef's paper](https://doi.org/10.1080/00031305.2012.687494) section 2.1.

For the univariate case, we need the derivative of our transformation function multiplied by our original standard error.

So, we need to find the derivative of x, g'.
This one is simple, but we could use symbolic differentiation in R:

```{r}
D(expression(log(x)), "x")
```

So: `g'(x) = 1/x`

We can apply the delta method here as our derivative of the transformation
times the original standard error:
e.g. [Ver Hoef's paper](https://doi.org/10.1080/00031305.2012.687494) Eqn 2. (but here
for the SD not the variance)

```{r}
g_theta_se <- (1 / theta) * theta_se
g_theta_se
```

Which matches our simulation-based check:

```{r}
sd(log(param_samples))
```

We can use that to draw our 95% CI:

```{r}
.q <- qnorm(0.975)
.q
hist(log(param_samples), breaks = 80)
abline(v = g_theta, col = "red")
abline(v = g_theta - .q * g_theta_se, col = "blue")
abline(v = g_theta + .q * g_theta_se, col = "blue")
```

The multivariate case (say combining multiple parameters) is a bit more
complex because we have covariance to account for, but the principle is the
same. See the file 'delta-method-multivariate'.
